# run_llama_server.sh 功能说明文档

`run_llama_server` 是一个自动化推理启动脚本，能够自动定位 Ollama 管理的模型文件，并以优化的参数启动 `llama-server`。

---

## 1. 核心功能

- **自动化路径映射**：通过模型关键字自动获取 Ollama 内部复杂的 Blob 路径。
- **参数灵活化**：支持在命令行直接指定端口和上下文长度，无需修改脚本。
- **网络自动绑定**：自动执行 `export LLAMA_ARG_HOST=0.0.0.0`，确保服务可被远程访问。
- **全量 GPU 加速**：默认开启 `--n-gpu-layers 999`，尝试将模型全部载入显存。

---

## 2. 参数规划

脚本采用位置参数设计，格式如下：
`run_llama_server <模型关键字> [端口] [上下文] [程序路径]`

| 位置 | 参数名 | 必填 | 缺省值 | 说明 |
| :--- | :--- | :--- | :--- | :--- |
| `$1` | 模型关键字 | **是** | - | 用于 `ollama_blobs` 过滤的名称 |
| `$2` | 端口 | 否 | `8080` | llama-server 监听的端口 |
| `$3` | 上下文 | 否 | `8192` | 模型推理的上下文长度 (`-c`) |
| `$4` | 程序路径 | 否 | `~/llama.cpp/build/bin/llama-server` | 可执行文件的绝对路径 |

---

## 3. 使用示例

### 示例 1：最简启动
使用缺省参数启动 qwen 模型（8080 端口，8192 上下文）：
```bash
run_llama_server qwen
```
